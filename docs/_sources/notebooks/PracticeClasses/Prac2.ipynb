{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939faaf7",
   "metadata": {},
   "source": [
    "# Probability Generating Functions, Practical Class 2\n",
    "\n",
    "\n",
    "**AMSI 2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa0510",
   "metadata": {},
   "source": [
    "1. Suppose a random integer-valued variable $X$ is generated as follows:\n",
    "   - With probability $0.6$, $X$ has probability generating function $\\mu_1(x)$.\n",
    "   - With probability $0.4$, $X$ has probability generating function $\\mu_2(x)$.\n",
    "\n",
    "   Independently of this choice, let $Y$ be a Bernoulli$(0.5)$ random variable (so $Y=0$ or $1$ with equal probability), and define $Z = X + Y$.\n",
    "\n",
    "   **(a)** Find the PGFs for $X$ and $Y$ in terms of $\\mu_1(x)$ and $\\mu_2(x)$.\n",
    "   \n",
    "   **(b)** Write down the probability generating function of $Z$.\n",
    " \n",
    "   **(c)** Explain how the coefficient of $x^k$ in this generating function can be interpreted in terms of the underlying random mechanism that produces $Z$.\n",
    "\n",
    "2. Let $D$ be the number of customers arriving at a store in one hour, with PGF $\\mu_D(x)$.  Each customer independently purchases a random number of items, with PGF $\\mu_S(x)$.  Let $T$ be the total number of items purchased in that hour and $\\psi(x)$ be the PGF of $T$.\n",
    "\n",
    "   **(a)** Express $\\psi(x)$ in terms of $\\mu_D(x)$ and $\\mu_S(x)$.\n",
    "\n",
    "   **(b)** Suppose\n",
    "\n",
    "   $$\n",
    "   \\mu_D(x) = 0.3 + 0.7x\n",
    "   $$\n",
    "   and\n",
    "\n",
    "   $$\n",
    "   \\mu_S(x) = 0.2 + 0.8x^2.\n",
    "   $$\n",
    "   Compute $[x^2]\\psi(x)$, and explain what this coefficient represents in the context of the problem.\n",
    "3. A traffic light allows a random number of cars to pass during each cycle.  The PGF for the number of cars passing in a single cycle is $\\mu(x)$. Assume different cycles are independent.\n",
    "\n",
    "   **(a)** Write down the probability generating function for the total number of cars passing through the light over two consecutive cycles.\n",
    "\n",
    "   **(b)** Explain how you would use this generating function to find the probability that exactly $n$ cars pass in two cycles.\n",
    "\n",
    "   **(c)** Describe how your approach generalises to more cycles.\n",
    "\n",
    "3. Consider a Galton-Watson process whose offspring distribution has PGF $\\mu(x)$.  Assume that $X_0=1$, $X_1=3$, and $X_2=4$.\n",
    "\n",
    "   **(a)** Find the PGF for $X_3$ conditional on the information above in terms of $\\mu(x)$.\n",
    "   \n",
    "   **(b)** The formula you found in (a) should depend on $X_2$.  Does knowing $X_1$ (or $X_0$) provide any useful information for predicting $X_3$?  \n",
    "\n",
    "   **(c)** For each of the following distributions, find a formula for $\\mathbb{P}[X_3 = n]$ for $n=0,1,2,\\ldots$.  Note that in each case a power of the PGF is a PGF of the same general form, so its series expansion is known:\n",
    "\n",
    "      *(i*). Poisson: $\\mu(x) = e^{\\lambda (x-1)} = \\sum_{k=0}^\\infty \\frac{e^{-\\lambda} (\\lambda x)^k}{k!}$.\n",
    "\n",
    "      *(ii)* Binomial: $\\mu(x) = (1-p + px)^N = \\sum_{k=0}^N \\binom{N}{k} p^k(1-p)^{N-k}x^k$.\n",
    "\n",
    "      *(iii)* Negative Binomial: $\\mu(x) = \\left(\\frac{p}{1-(1-p)x}\\right)^r = \\sum_{N=0}^\\infty \\binom{k+r-1}{k} p^{r}(1-p)^k x^k$.\n",
    "\n",
    "      *(iv)* Why is it harder to do this for the Geometric Distribution: $\\mu(x)=\\frac{1}{1-(1-p)x} = \\sum_{k=1}^\\infty (1-p)^{k-1} p x^k$? \n",
    "      \n",
    "      *(v)* (*) Write down an intgral that can be used in a general case where a power of the PGF is not simply another PGF whose Taylor Series is known.\n",
    "\n",
    "\n",
    "5. Consider a Galtonâ€“Watson branching process with offspring distribution having probability generating function  \n",
    "\n",
    "   $$\n",
    "   \\mu(x) = p_0 + p_1 x + p_2 x^2\n",
    "   $$\n",
    "\n",
    "   where $p_0 = 0.2$, $p_1 = 0.4$, $p_2 = 0.4$.\n",
    "\n",
    "   **(a)** Let $\\alpha$ denote the probability that the population eventually goes extinct. Write down the equation that $\\alpha$ must satisfy in terms of $\\mu(x)$.\n",
    "\n",
    "   **(b)** Solve this equation and identify which solution in $[0,1]$ is the correct extinction probability. Explain why any other solutions should be rejected.\n",
    "\n",
    "   **(c)** Compute the mean number of offspring $\\mu'(1)$ and use it to explain whether extinction is certain or only occurs with some probability less than $1$.\n",
    "\n",
    "\n",
    "\n",
    "6. Consider a continuous-time branching process defined as follows:\n",
    "   \n",
    "   - Each individual lives for an exponentially distributed amount of time with rate $r$.  \n",
    "\n",
    "   - When an individual dies, it is instantaneously replaced by a random number of offspring with probability generating function $\\hat{\\mu}(x)$.  \n",
    "   \n",
    "   - All individuals behave independently of one another.\n",
    "\n",
    "   Let $\\Phi(x,t)$ be the probability generating function of the population size $X(t)$ at time $t$, starting from a single individual at time $0$: $X(0)=1$.  \n",
    "\n",
    "   The Backward Kolmogorov equation for this process is  \n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial t}\\Phi(x,t) = r\\bigl(\\hat{\\mu}(\\Phi(x,t)) - \\Phi(x,t)\\bigr),\n",
    "   $$\n",
    "\n",
    "   with initial condition $\\Phi(x,0) = x$.\n",
    "\n",
    "   Define  \n",
    "\n",
    "   $$\n",
    "   \\alpha(t) = \\mathbb{P}[\\text{the population is extinct by time } t] = \\Phi(0,t).\n",
    "   $$\n",
    "\n",
    "\n",
    "   **(a)** By substituting $x=0$, derive a differential equation satisfied by $\\alpha(t)$.\n",
    "\n",
    "   **(b)** Suppose the offspring distribution is Poisson with mean $2$, so that  \n",
    "   \n",
    "   $$\n",
    "   \\hat{\\mu}(x) = e^{2(x-1)}.\n",
    "   $$\n",
    "\n",
    "   Write the resulting ODE for $\\alpha(t)$ explicitly.\n",
    "\n",
    "   **(c)** Explain what the limit  \n",
    "\n",
    "   $$\n",
    "   \\lim_{t \\to \\infty} \\alpha(t)\n",
    "   $$\n",
    "\n",
    "   represents.\n",
    "\n",
    "7. Review the derivation of $\\Phi(x,T_1+T_2)$ in the notes.  Adapt it to find the generation-based equivalent for $\\Phi_{g_1+g_2}(x)$.\n",
    "\n",
    "8. We derived the Forward and Backward Kolmogorov Equations for the case of one initial individual.  Assume instead that the process begins with $2$ individuals.\n",
    "\n",
    "   **(a)** What would need to change about the setup of the Forward Kolmogorov Equation derivation?\n",
    "\n",
    "   **(b)** What would need to change about the setup of the Backward Kolmogorov Equation derivation?\n",
    "\n",
    "   **(c)** If you have time, derive the two equations.\n",
    "\n",
    "9. Consider the two different definitions of the Continuous-Time Galton-Watson processes.  Assume we are using the definition based on individuals in which they have an exponentially distributed lifetime of rate $r$, corresponding to a Poisson process with rate $r$.  By referring to superposition[^compound] of Poisson processes, explain why at time $t_i$ we can assume that the time until the next event is exponentially distributed with rate $rX(t_i)$.\n",
    "\n",
    "[^compound]: It's embarrassing, but I suddenly realized while writing this question that I've been using \"compound Poisson Process\" where I mean \"superposition of Poisson processes\".  I have corrected this in the notes.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016d988",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
